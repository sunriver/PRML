{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 参考方法"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:53:42.901604Z",
     "start_time": "2019-08-16T08:53:42.875140Z"
    }
   },
   "outputs": [],
   "source": [
    "#https://www.cnblogs.com/vipyoumay/p/ner-chinese-keras.html\n",
    "import numpy\n",
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "import pickle\n",
    "import platform\n",
    "\n",
    "def load_data():\n",
    "    train = _parse_data(open('./train_data.data', 'rb'))\n",
    "    test = _parse_data(open('./test_data.data', 'rb'))\n",
    "#     print(train)\n",
    "\n",
    "    word_counts = Counter(row[0].lower() for sample in train for row in sample)\n",
    "    vocab = [w for w, f in iter(word_counts.items()) if f >= 2]\n",
    "    chunk_tags = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "    # save initial config data\n",
    "    with open('model/config.pkl', 'wb') as outp:\n",
    "        pickle.dump((vocab, chunk_tags), outp)\n",
    "\n",
    "    train = _process_data(train, vocab, chunk_tags)\n",
    "    test = _process_data(test, vocab, chunk_tags)\n",
    "    return train, test, (vocab, chunk_tags)\n",
    "\n",
    "\n",
    "def _parse_data(fh):\n",
    "    #  in windows the new line is '\\r\\n\\r\\n' the space is '\\r\\n' . so if you use windows system,\n",
    "    #  you have to use recorsponding instructions\n",
    "\n",
    "    if platform.system() == 'Windows':\n",
    "        split_text = '\\r\\n'\n",
    "    else:\n",
    "        split_text = '\\n'\n",
    "\n",
    "\n",
    "    #样本是\\n\\n分割\n",
    "    string = fh.read().decode('utf-8')\n",
    "    print(string)\n",
    "    data = [[row.split() for row in sample.split(split_text)] for\n",
    "            sample in\n",
    "            string.strip().split(split_text + split_text)]\n",
    "    fh.close()\n",
    "    return data\n",
    "\n",
    "\n",
    "def _process_data(data, vocab, chunk_tags, maxlen=None, onehot=False):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(s) for s in data)\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]  # set to <unk> (index 1) if not in vocab\n",
    "\n",
    "    y_chunk = [[chunk_tags.index(w[1]) for w in s] for s in data]\n",
    "\n",
    "    x = pad_sequences(x, maxlen)  # left padding\n",
    "\n",
    "    y_chunk = pad_sequences(y_chunk, maxlen, value=-1)\n",
    "\n",
    "    if onehot:\n",
    "        y_chunk = numpy.eye(len(chunk_tags), dtype='float32')[y_chunk]\n",
    "    else:\n",
    "        y_chunk = numpy.expand_dims(y_chunk, 2) #扩展维度\n",
    "    return x, y_chunk\n",
    "\n",
    "\n",
    "def process_data(data, vocab, maxlen=100):\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [word2idx.get(w[0].lower(), 1) for w in data]\n",
    "    length = len(x)\n",
    "    x = pad_sequences([x], maxlen)  # left padding\n",
    "    return x, length\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:54:52.435812Z",
     "start_time": "2019-08-16T08:54:49.582635Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IOPub data rate exceeded.\n",
      "The notebook server will temporarily stop sending output\n",
      "to the client in order to avoid crashing it.\n",
      "To change this limit, set the config variable\n",
      "`--NotebookApp.iopub_data_rate_limit`.\n",
      "\n",
      "Current values:\n",
      "NotebookApp.iopub_data_rate_limit=1000000.0 (bytes/sec)\n",
      "NotebookApp.rate_limit_window=3.0 (secs)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# train, test, (vocab, chunk_tags) = load_data()\n",
    "\n",
    "train = _parse_data(open('./train_data.data', 'rb'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-16T08:56:52.085888Z",
     "start_time": "2019-08-16T08:56:52.076579Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['当', 'O'], ['希', 'O'], ['望', 'O'], ['工', 'O'], ['程', 'O'], ['救', 'O'], ['助', 'O'], ['的', 'O'], ['百', 'O'], ['万', 'O'], ['儿', 'O'], ['童', 'O'], ['成', 'O'], ['长', 'O'], ['起', 'O'], ['来', 'O'], ['，', 'O'], ['科', 'O'], ['教', 'O'], ['兴', 'O'], ['国', 'O'], ['蔚', 'O'], ['然', 'O'], ['成', 'O'], ['风', 'O'], ['时', 'O'], ['，', 'O'], ['今', 'O'], ['天', 'O'], ['有', 'O'], ['收', 'O'], ['藏', 'O'], ['价', 'O'], ['值', 'O'], ['的', 'O'], ['书', 'O'], ['你', 'O'], ['没', 'O'], ['买', 'O'], ['，', 'O'], ['明', 'O'], ['日', 'O'], ['就', 'O'], ['叫', 'O'], ['你', 'O'], ['悔', 'O'], ['不', 'O'], ['当', 'O'], ['初', 'O'], ['！', 'O']]\n",
      "\n",
      "\n",
      "['当', 'O']\n"
     ]
    }
   ],
   "source": [
    "print(train[0])\n",
    "print('\\n')\n",
    "print(train[0][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-23T05:51:22.989817Z",
     "start_time": "2019-09-23T05:51:19.847704Z"
    },
    "pixiedust": {
     "displayParams": {}
    }
   },
   "outputs": [],
   "source": [
    "# %%pixie_debuggerger\n",
    "def parse_file(file_name):\n",
    "    with open(file_name, 'rb') as file:\n",
    "        content = file.read().decode(\"utf-8\").strip()\n",
    "        samples = content.split('\\n\\n')\n",
    "        sample_rows = [sample.split('\\n') for sample in samples] \n",
    "        data = [[row.split() for row  in sample ] for sample in sample_rows]\n",
    "        file.close()\n",
    "        return data\n",
    "    return []\n",
    "\n",
    "def process(data, vocab, chunk_tags, maxlen=None, onehot=False):\n",
    "    if maxlen is None:\n",
    "        maxlen = max(len(sample) for sample in data)\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [[word2idx.get(w[0].lower(), 1) for w in s] for s in data]  # set to <unk> (index 1) if not in vocab\n",
    "\n",
    "    y_chunk = [[chunk_tags.index(w[1]) for w in s] for s in data]\n",
    "\n",
    "    x = pad_sequences(x, maxlen)  # left padding\n",
    "\n",
    "    y_chunk = pad_sequences(y_chunk, maxlen, value=-1) #https://www.twblogs.net/a/5c113708bd9eee5e40bb23af\n",
    "\n",
    "    if onehot:\n",
    "        y_chunk = numpy.eye(len(chunk_tags), dtype='float32')[y_chunk]\n",
    "    else:\n",
    "        y_chunk = numpy.expand_dims(y_chunk, 2) #扩展维度\n",
    "    return x, y_chunk\n",
    "    \n",
    "train = parse_file('train_data.data')\n",
    "test = parse_file('test_data.data')\n",
    "# print(train_data[0])\n",
    "# print('\\n')\n",
    "# print(test_data[0][0])\n",
    "# test_data = parse_file('test_data.data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T03:13:52.481620Z",
     "start_time": "2019-08-19T03:13:49.759166Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<generator object <genexpr> at 0x153a12c78>\n"
     ]
    }
   ],
   "source": [
    "# row[0].lower for sample in train for row in sample\n",
    "\n",
    "# words = ((row[0].lower() for row in sample) for sample in train)\n",
    "# # print(words)\n",
    "# words_counts = Counter(words)\n",
    "# print(words_counts)\n",
    "\n",
    "import pickle\n",
    "\n",
    "words = (row[0].lower()for sample in train for row in sample)\n",
    "print(words)\n",
    "words_counts = Counter(words)\n",
    "\n",
    "vocab = [w for w, f in iter(words_counts.items()) if f > 2 ]\n",
    "\n",
    "chunk_tags = ['O', 'B-PER', 'I-PER', 'B-LOC', 'I-LOC', \"B-ORG\", \"I-ORG\"]\n",
    "\n",
    "\n",
    "with open('config.pkl', 'wb') as outp:\n",
    "      pickle.dump((vocab, chunk_tags), outp)\n",
    "\n",
    "\n",
    "# print(words_counts)\n",
    "train_X, train_y = process(train, vocab, chunk_tags)\n",
    "test_X, test_y = process(test, vocab, chunk_tags)\n",
    "# return train, test, (vocab, chunk_tags)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T03:13:58.160645Z",
     "start_time": "2019-08-19T03:13:57.203236Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "embedding_3 (Embedding)      (None, None, 200)         787400    \n",
      "_________________________________________________________________\n",
      "bidirectional_3 (Bidirection (None, None, 200)         240800    \n",
      "_________________________________________________________________\n",
      "crf_3 (CRF)                  (None, None, 7)           1470      \n",
      "=================================================================\n",
      "Total params: 1,029,670\n",
      "Trainable params: 1,029,670\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.7/site-packages/keras_contrib/layers/crf.py:346: UserWarning: CRF.loss_function is deprecated and it might be removed in the future. Please use losses.crf_loss instead.\n",
      "  warnings.warn('CRF.loss_function is deprecated '\n",
      "/usr/local/lib/python3.7/site-packages/keras_contrib/layers/crf.py:353: UserWarning: CRF.accuracy is deprecated and it might be removed in the future. Please use metrics.crf_accuracy\n",
      "  warnings.warn('CRF.accuracy is deprecated and it '\n"
     ]
    }
   ],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Bidirectional\n",
    "from keras_contrib.layers import CRF\n",
    "\n",
    "from keras.layers.recurrent import LSTM\n",
    "\n",
    "EMBED_DIM = 200\n",
    "BiRNN_UNITS = 200\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Embedding(len(vocab), EMBED_DIM, mask_zero=True))  # Random embedding\n",
    "#双斜杠（//）表示地板除，即先做除法（/），然后向下取整（floor)\n",
    "model.add(Bidirectional(LSTM(BiRNN_UNITS // 2, return_sequences=True)))\n",
    "crf = CRF(len(chunk_tags), sparse_target=True)\n",
    "model.add(crf)\n",
    "model.summary()\n",
    "model.compile('adam', loss=crf.loss_function, metrics=[crf.accuracy])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T04:05:29.593126Z",
     "start_time": "2019-08-19T03:14:33.002520Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/local/lib/python3.7/site-packages/tensorflow/python/ops/math_ops.py:3066: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Train on 50658 samples, validate on 4631 samples\n",
      "Epoch 1/5\n",
      " - 787s - loss: 4.0529 - crf_viterbi_accuracy: 0.9547 - val_loss: 7.8834 - val_crf_viterbi_accuracy: 0.9698\n",
      "Epoch 2/5\n",
      " - 630s - loss: 3.9702 - crf_viterbi_accuracy: 0.9785 - val_loss: 7.8646 - val_crf_viterbi_accuracy: 0.9749\n",
      "Epoch 3/5\n",
      " - 558s - loss: 3.9572 - crf_viterbi_accuracy: 0.9846 - val_loss: 7.8613 - val_crf_viterbi_accuracy: 0.9766\n",
      "Epoch 4/5\n",
      " - 541s - loss: 3.9511 - crf_viterbi_accuracy: 0.9883 - val_loss: 7.8606 - val_crf_viterbi_accuracy: 0.9781\n",
      "Epoch 5/5\n",
      " - 539s - loss: 3.9469 - crf_viterbi_accuracy: 0.9913 - val_loss: 7.8592 - val_crf_viterbi_accuracy: 0.9788\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1414f8908>"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(train_X, train_y, batch_size=16,epochs=5, validation_data=[test_X, test_y], verbose=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# predict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-08-19T07:22:12.725844Z",
     "start_time": "2019-08-19T07:22:12.538423Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "raw1------\n",
      "[[1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 0. 1. 0. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [0. 0. 0. 0. 1. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]\n",
      " [0. 1. 0. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [0. 0. 1. 0. 0. 0. 0.]\n",
      " [1. 0. 0. 0. 0. 0. 0.]]\n",
      "result------\n",
      "[0, 0, 0, 1, 2, 2, 2, 2, 2, 2, 2, 2, 0, 0, 0, 0, 0, 0, 3, 4, 4, 4, 4, 0, 0, 1, 2, 2, 0]\n",
      "result_tags------\n",
      "['O', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'I-PER', 'O', 'O', 'O', 'O', 'O', 'O', 'B-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'I-LOC', 'O', 'O', 'B-PER', 'I-PER', 'I-PER', 'O']\n",
      "['person: 米哈伊尔-普罗霍夫 蔡崇信', 'location: 巴克莱中心', 'organzation:']\n"
     ]
    }
   ],
   "source": [
    "def process_predict_data(data, vocab, maxlen=100):\n",
    "    word2idx = dict((w, i) for i, w in enumerate(vocab))\n",
    "    x = [word2idx.get(w[0].lower(), 1) for w in data]\n",
    "    length = len(x)\n",
    "    x = pad_sequences([x], maxlen)  # left padding\n",
    "    return x, length\n",
    "\n",
    "# predict_text = '中华人民共和国国务院总理周恩来在外交部长陈毅的陪同下，连续访问了埃塞俄比亚等非洲10国以及阿尔巴尼亚'\n",
    "# predict_text = '香港是中华人民共和国两个特别行政区之一，位于南海北岸、珠江口东侧，北接广东省深圳市，西面与邻近的澳门特别行政区相距63公里，其余两面与南海邻接'\n",
    "predict_text = '今日，米哈伊尔-普罗霍夫宣布将篮网和巴克莱中心卖给蔡崇信。'\n",
    "sstr, length = process_predict_data(predict_text, vocab)\n",
    "\n",
    "\n",
    "raw = model.predict(sstr)\n",
    "# print(raw)\n",
    "raw1 = raw[0][-length:]\n",
    "print(\"raw1------\")\n",
    "print(raw1)\n",
    "\n",
    "result = [np.argmax(row) for row in raw1] #获取最大值对应的位置索引\n",
    "\n",
    "print(\"result------\")\n",
    "print(result)\n",
    "result_tags = [chunk_tags[i] for i in result]\n",
    "print(\"result_tags------\")\n",
    "print(result_tags)\n",
    "per, loc, org = '', '', ''\n",
    "\n",
    "for s, t in zip(predict_text, result_tags):\n",
    "    if t in ('B-PER', 'I-PER'):\n",
    "        per += ' ' + s if (t == 'B-PER') else s\n",
    "    if t in ('B-ORG', 'I-ORG'):\n",
    "        org += ' ' + s if (t == 'B-ORG') else s\n",
    "    if t in ('B-LOC', 'I-LOC'):\n",
    "        loc += ' ' + s if (t == 'B-LOC') else s\n",
    "\n",
    "print(['person:' + per, 'location:' + loc, 'organzation:' + org])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "372px",
    "left": "1020px",
    "right": "20px",
    "top": "120px",
    "width": "380px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
