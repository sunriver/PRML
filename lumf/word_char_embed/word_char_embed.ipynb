{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# import lib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 参考例子https://github.com/sunriver/keras-word-char-embd/blob/master/keras_wc_embd/wrapper.py\n",
    "# 使用Keras 函数式API搭建模型"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# load data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# preprocess data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T06:27:25.128076Z",
     "start_time": "2019-09-29T06:27:25.118638Z"
    }
   },
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import one_hot\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T06:27:27.086764Z",
     "start_time": "2019-09-29T06:27:27.061879Z"
    },
    "run_control": {
     "marked": false
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "#step 0\n",
    "sentences = [\n",
    "    ['All', 'work', 'and', 'no', 'play'],\n",
    "    ['makes', 'Jack', 'a', 'dull', 'boy'],\n",
    "     ['Please', 'tell', 'me', 'how', 'to', 'play', 'this', 'game'],\n",
    "]\n",
    "\n",
    "# sentences_nums = len(sentences)\n",
    "\n",
    "\n",
    "def onehot_and_pad_words(sentences : list):\n",
    "    \"\"\"\n",
    "       sentences : list of sentence which consists of word\n",
    "       #0所有句子->1.词汇集合vocab->2.词汇Index(word, index)->3.将每个句子转化成OneHot序列下标 ->4.序列对齐\n",
    "    \"\"\"\n",
    "    #step1 词汇集合\n",
    "    word_counts = Counter(word.lower() for sen in sentences for word in sen)\n",
    "    vocab = [word for word, count in word_counts.items() if count > 0 ]\n",
    "\n",
    "    #step2 词汇索引\n",
    "    word2ids = dict((word,index) for index, word in enumerate(vocab))\n",
    "#     print(word2ids)\n",
    "\n",
    "    #step3 将每个句子转化成OneHot序列下标\n",
    "    X_sentences = [[ word2ids.get(word, 1) for word in sentence] for sentence in sentences]\n",
    "#     print(X_sentences)\n",
    "\n",
    "    #4.Order Label向量对齐\n",
    "    max_sentence_len = max(map(len, sentences))\n",
    "    word_seqs = pad_sequences(X_sentences, max_sentence_len)\n",
    "    \n",
    "    sentences_num = len(sentences)\n",
    "    return (sentences_num, max_sentence_len), np.asarray(word_seqs), len(vocab)\n",
    "\n",
    "\n",
    "\n",
    "def onehot_and_pad_chars(sentences : list):\n",
    "    #step1 创建char集合\n",
    "    char_counts = Counter(char.lower() for sentence in sentences for word in sentence for char in word)\n",
    "    print(char_counts)\n",
    "    chars_set = [char for char, count in char_counts.items()]\n",
    "\n",
    "    #step2 创建<Char，索引>字典\n",
    "    char2ids = dict((word, index) for index, word in enumerate(chars_set))\n",
    "    \n",
    "    #step3 将句子映射为字符索引序列,由于是3维数据，所以不能用pad_sequence对齐，所以下面手动对齐\n",
    "    sentences_num = len(sentences)\n",
    "    sentences_max_len = max(map(len, sentences))\n",
    "    words_max_len = max(len(word) for sentence in sentences for word in sentence)\n",
    "#     print('sentences_max_len={}, words_max_len={}'.format(sentences_max_len, words_max_len))\n",
    "    \n",
    "    chars_seqs = [[[0] * words_max_len for i in range(sentences_max_len)] for j in range(sentences_num)]\n",
    "#     chars_seqs = [[[char2ids.get(char.lower()) for char in word ] for word in sentence] for sentence in sentences]\n",
    "#     display(chars_seqs)\n",
    "    \n",
    "    for i, sentence in enumerate(sentences):\n",
    "        for j, word in enumerate(sentence):\n",
    "            for k, char in enumerate(word):\n",
    "                char_index = char2ids.get(char.lower())\n",
    "#                 print(\"i={} sentence={}; j={} word={}; k={}, char={}. index={}\".format(i, sentence, j, word, k, char, char_index))\n",
    "                chars_seqs[i][j][k] = char_index\n",
    "                \n",
    "    \n",
    "#     display(chars_seqs)\n",
    "    return (sentences_num, sentences_max_len, words_max_len), np.asarray(chars_seqs), len(chars_set)\n",
    "\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# define model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T06:27:29.536324Z",
     "start_time": "2019-09-29T06:27:29.529688Z"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from keras.models import Model\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import Softmax\n",
    "from keras.layers import Bidirectional\n",
    "from keras.layers import LSTMCell\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Input\n",
    "from keras.layers import Concatenate\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import Dense\n",
    "from keras.optimizers import Adam\n",
    "from keras.losses import categorical_crossentropy\n",
    "from keras.metrics import categorical_accuracy\n",
    "from keras.losses import sparse_categorical_crossentropy\n",
    "from keras.metrics import sparse_categorical_accuracy\n",
    "from keras.callbacks import TensorBoard\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T06:33:13.003544Z",
     "start_time": "2019-09-29T06:33:11.426815Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counter({'a': 9, 'l': 9, 'e': 6, 'o': 5, 'k': 3, 'p': 3, 'y': 3, 'm': 3, 's': 3, 't': 3, 'w': 2, 'n': 2, 'd': 2, 'h': 2, 'r': 1, 'j': 1, 'c': 1, 'u': 1, 'b': 1, 'i': 1, 'g': 1})\n",
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_char (InputLayer)         (None, 8, 6)         0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_word (InputLayer)         (None, 8)            0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embed_char (Embedding)          (None, 8, 6, 15)     315         input_char[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "embed_word (Embedding)          (None, 8, 20)        340         input_word[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed (TimeDistribut (None, 8, 30)        3720        embed_char[0][0]                 \n",
      "__________________________________________________________________________________________________\n",
      "concate (Concatenate)           (None, 8, 50)        0           embed_word[0][0]                 \n",
      "                                                                 time_distributed[0][0]           \n",
      "__________________________________________________________________________________________________\n",
      "LSTM2 (LSTM)                    (None, 5)            1120        concate[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "Softmax (Dense)                 (None, 2)            12          LSTM2[0][0]                      \n",
      "==================================================================================================\n",
      "Total params: 5,507\n",
      "Trainable params: 5,507\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "#input_shape\n",
    "def get_inputs_outputs_layer2(\n",
    "     word_dict_len,\n",
    "     doc_words_max_len,\n",
    "     char_dict_len,\n",
    "     word_chars_max_len\n",
    "    ):\n",
    "    \n",
    "    input_word = Input(shape = (doc_words_max_len, ), name='input_word')\n",
    "    layer_word_embed = Embedding(input_dim = word_dict_len, output_dim = 20, name=\"embed_word\")(input_word)\n",
    "        \n",
    "    \n",
    "    input_char = Input(shape=(doc_words_max_len, word_chars_max_len), name = 'input_char')\n",
    "    char_embed_out_dim = 15\n",
    "    layer_char_embed = Embedding(input_dim = char_dict_len, output_dim = char_embed_out_dim, name=\"embed_char\")(input_char)     \n",
    "    layer_char_bilstm = Bidirectional(LSTM(units = 15, input_shape=(doc_words_max_len, word_chars_max_len, char_embed_out_dim)))\n",
    "    \n",
    "    layer_char_timedistributed = TimeDistributed(layer=layer_char_bilstm, name = \"time_distributed\")(layer_char_embed)\n",
    "        \n",
    "    layer_concat = Concatenate(name = 'concate')([layer_word_embed, layer_char_timedistributed])\n",
    "    #3dim -> 2dim， 从高维向低维过度\n",
    "    layer_lstm2 = LSTM(units=5, name='LSTM2')(layer_concat)\n",
    "    layer_softmax = Dense(units=2, activation='softmax', name='Softmax')(layer_lstm2)\n",
    "    \n",
    "    return [input_word, input_char], layer_softmax\n",
    "\n",
    "\n",
    "#batch_shape\n",
    "def get_inputs_outputs_layer(\n",
    "     word_dict_len,\n",
    "     doc_words_max_len,\n",
    "     char_dict_len,\n",
    "     word_chars_max_len\n",
    "    ):\n",
    "\n",
    "    input_word = Input(batch_shape = (None, doc_words_max_len), name='input_word')\n",
    "    layer_word_embed = Embedding(input_dim = word_dict_len, output_dim = 20, name=\"embed_word\")(input_word)\n",
    "        \n",
    "  \n",
    "    \n",
    "    input_char = Input(batch_shape=(None,  doc_words_max_len, word_chars_max_len), name = 'input_char')\n",
    "    char_embed_out_dim = 15\n",
    "    layer_char_embed = Embedding(input_dim = char_dict_len, output_dim = char_embed_out_dim, name=\"embed_char\")(input_char)     \n",
    "    layer_char_bilstm = Bidirectional(LSTM(units = 15, input_shape=(doc_words_max_len, word_chars_max_len, char_embed_out_dim)))\n",
    "    \n",
    "    layer_char_timedistributed = TimeDistributed(layer=layer_char_bilstm, name = \"time_distributed\")(layer_char_embed)\n",
    "        \n",
    "    layer_concat = Concatenate(name = 'concate')([layer_word_embed, layer_char_timedistributed])\n",
    "    #3dim -> 2dim， 从高维向低维过度\n",
    "    layer_lstm2 = LSTM(units=5, name='LSTM2')(layer_concat)\n",
    "    layer_softmax = Dense(units=2, activation='softmax', name='Softmax')(layer_lstm2)\n",
    "    \n",
    "    return [input_word, input_char], layer_softmax\n",
    "\n",
    "\n",
    "(sentences_num, max_sentence_len), word_seqs, word_dict_len = onehot_and_pad_words(sentences)\n",
    "\n",
    "(sentences_num, sentences_max_len, words_max_len), chars_seqs, char_dict_len = onehot_and_pad_chars(sentences)\n",
    "\n",
    "# inputs, outputs = get_inputs_outputs_layer(word_dict_len, max_sentence_len, char_dict_len, words_max_len)\n",
    "\n",
    "inputs, outputs = get_inputs_outputs_layer2(word_dict_len, max_sentence_len, char_dict_len, words_max_len)\n",
    "\n",
    "model = Model(inputs = inputs, outputs = outputs)\n",
    "\n",
    "#注意这里设置的loss和metrics和softmax层对应的，softmax 输出两个值\n",
    "#https://blog.csdn.net/qq_20011607/article/details/89213908\n",
    "model.compile(optimizer = Adam(), loss = sparse_categorical_crossentropy, metrics=[sparse_categorical_accuracy])\n",
    "model.summary()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2019-09-29T06:33:46.022438Z",
     "start_time": "2019-09-29T06:33:30.423915Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/1\n",
      "200/200 [==============================] - 4s 21ms/step - loss: 0.1801 - sparse_categorical_accuracy: 0.9300\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x14fcbe0f0>"
      ]
     },
     "execution_count": 71,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "word_seqs = np.asarray(word_seqs)\n",
    "chars_seqs = np.asarray(chars_seqs)\n",
    "labels = np.asarray([ 0, 1, 1])\n",
    "\n",
    "def batch_generator():\n",
    "    while True:\n",
    "   \n",
    "        yield [word_seqs, chars_seqs], labels\n",
    "\n",
    "\n",
    "def get_callbacks():\n",
    "    cb_borad = TensorBoard(log_dir='./logs',  # log 目录\n",
    "                 histogram_freq=0,  # 按照何等频率（epoch）来计算直方图，0为不计算\n",
    "#                  batch_size=32,     # 用多大量的数据计算直方图\n",
    "                 write_graph=True,  # 是否存储网络结构图\n",
    "                 write_grads=True, # 是否可视化梯度直方图\n",
    "                 write_images=True,# 是否可视化参数\n",
    "                 embeddings_freq=0, \n",
    "                 embeddings_layer_names=None, \n",
    "                 embeddings_metadata=None)\n",
    "    return [cb_borad]\n",
    "    \n",
    "model.fit_generator(\n",
    "    generator=batch_generator(),\n",
    "    steps_per_epoch=200,\n",
    "    epochs=1,\n",
    "    callbacks= get_callbacks()\n",
    ")\n",
    "\n",
    "# model.fit(np.asarray([np.asarray(word_seqs), np.asarray(chars_seqs)]), np.asarray([0, 1]),\n",
    "#           steps_per_epoch=200,\n",
    "#           epochs=1\n",
    "#          )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "165px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "position": {
    "height": "431px",
    "left": "1050px",
    "right": "20px",
    "top": "120px",
    "width": "350px"
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
